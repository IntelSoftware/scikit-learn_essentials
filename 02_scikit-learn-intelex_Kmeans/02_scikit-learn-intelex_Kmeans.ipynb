{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans Algorithm Using Intel® Extension for Scikit-learn*\n",
    "\n",
    "![Assets/kmeans.png](Assets/kmeans.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='Back-to-Sections'></a>\n",
    "# Sections\n",
    "- _Discussion:_ [Kmeans Algorithm](#Kmeans-Algorithm)\n",
    "- _Code:_ [Implementation of Kmeans targeting CPU using Intel Extension for Scikit-learn for Kmeans interactive](#Implementation-of-Kmeans-in-Batch-mode)\n",
    "- _Code:_ [Implementation of Kmeans targeting **Distributed CPU** using Intel Extension for Scikit-learn for Kmeans](#Implementation-of-Kmeans-using-Distributed-Processing)\n",
    "- _Code:_ [Implementation of Kmeans targeting **GPU** using Intel Extension for Scikit-learn for Kmeans](#Implementation-of-Kmeans-targeting-GPU)\n",
    "\n",
    "You will review, modify and execute code for unsupervised clustering of data using Intel Extension for Scikit-learn for Kmeans and DBSCAN on a single CPU, single Gpu, and distributed across multiple CPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "* Describe the value of Intel® Extension for Scikit-learn methodology in extending scikit-learn optimzation capabilites\n",
    "* Name key imports and function calls to use Intel Extension for Scikit-learn to target Kmeans for use on CPU, GPU and distributed CPU environments\n",
    "* Apply a single Daal4py function to enable Kmeans targeting CPU and GPU using SYCL context\n",
    "* Build a Sklearn implementation of Kmeans targeting CPU and GPU using Intel optimized Sklearn Extensions for Kmeans\n",
    "\n",
    "\n",
    "## Library Dependencies:\n",
    " - pip install pickle\n",
    " - also requires these libraries if they are not already installed: **matplotlib, numpy, pandas**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Intel Extension for Scikit-learn\n",
    "\n",
    "Intel® Extension for Scikit-learn contains drop-in replacement patching functionality for the Scikit-learn machine learning library for Python. The patches were originally available in the daal4py package. All future updates for the patching will be available only in Intel Extension for Scikit-learn. All performance claims obtained using daal4py are applicable for Intel Extension for Scikit-learn.\n",
    "\n",
    "The value of the patch is providing optimized versions of common Scikit-learn machine learning algorithms used for data science. An added value is the ability to invoke these functions on either CPU or GPU.\n",
    "\n",
    "Applying Intel(R) Extension for Scikit-learn will impact the following existing [scikit-learn algorithms:](https://intel.github.io/scikit-learn-intelex/algorithms.html)\n",
    "\n",
    "You can take advantage of the optimizations of Intel Extension for Scikit-learn by adding just two lines of code before the usual Scikit-learn imports:\n",
    "\n",
    " - from sklearnex import patch_sklearn\n",
    " - patch_sklearn()\n",
    "\n",
    " - from sklearn.cluster import KMeans\n",
    " - ... import other sklearn algoritms as needed ...\n",
    " \n",
    "Learn more about [various ways to patch](https://intel.github.io/scikit-learn-intelex/) scikit-learn very selectively or upon entire python scripts, or global patching, or even how to unpatch.\n",
    "\n",
    "Intel Extension for Scikit-learn uses Intel® oneAPI Data Analytics Library (oneDAL) to achieve its acceleration. The optimizations aim for the efficient use of CPU resources. The library enables all the latest vector instructions, such as Intel® Advanced Vector Extensions (Intel AVX-512). It also uses cache-friendly data blocking, fast Basic Linear Algebra Subprograms (BLAS) operations with Intel OneAPI Math Kernel Library (oneMKL), scalable multi-threading with Intel oneAPI Thread Building Blocks (oneTBB) library, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intel® oneAPI Data Analytics Library (oneDAL) aka daal4py\n",
    "As mentioned, Intel Extension for Scikit-learn uses Intel® oneAPI Data Analytics Library (oneDAL) under the hood to achieve its acceleration, and our general recommendation is to use Intel Extension for Scikit-learn whenever possible.  Most functionality found in Intel® oneAPI Data Analytics Library (oneDAL)  is exposed through the higher level interface, Intel Extension for Scikit-learn, and this is the preferred interface. However, there are a few funcctions found in Intel® oneAPI Data Analytics Library (oneDAL) not yet ported to Intel Extension for Scikit-learn so it is good to know how to leverage the functionality in either interface for now. For example, in the code below, we use daal4py to invoke the distributed compute mode for Kmeans.\n",
    "\n",
    "oneDAL has a Python API that is provided as a standalone Python library called daal4py.\n",
    "\n",
    "Daal4py, included in Intel® Distribution for Python* as part of the Intel® AI Analytics Toolkit, is an easy-to-use Python* API  that provides superior performance for your machine learning algorithms and frameworks. Designed for data scientists, it provides a simple way to utilize powerful Intel® DAAL machine learning algorithms in a flexible and customizable manner. For scaling capabilities, daal4py also provides you the option to process and analyze data via batch, streaming, or distributed processing modes, allowing you to choose the option to best fit your system's needs. \n",
    "\n",
    "The example below shows how daal4py can be used to calculate K-Means clusters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Kmeans Algorithm\n",
    "Kmeans is a clustering algorithm that partitions observations from a dataset into a requested number of geometric clusters of points closest to the cluster’s own center of mass. Using an initial estimate of the centroids, the algorithm iteratively updates the positions of the centroids until a fixed point.\n",
    "\n",
    "\n",
    "Kmeans is a simple and powerful ML algorithm to cluster data into similar groups. Its objective is to split a set of N observations into K clusters. This is achieved by minimizing inertia (i.e., the sum of squared Euclidian distances from observations to the cluster centers, or centroids). The algorithm is iterative, with two steps in each iteration:\n",
    "* For each observation, compute the distance from it to each centroid, and then reassign each observation to the cluster with the nearest centroid.\n",
    "* For each cluster, compute the centroid as the mean of observations assigned to this cluster.\n",
    "\n",
    "Repeat these steps until the number of iterations exceeds a predefined maximum or the algorithm converges (i.e., the difference between two consecutive inertias is less than a predefined threshold).\n",
    "Different methods are used to get initial centroids for the first iteration. The algorithm can select random observations as initial centroids or use more complex methods such as kmeans\n",
    "\n",
    "- [Back to Sections](#Back-to-Sections)\n",
    "\n",
    "### About the data\n",
    "The data included in these exercises was built seperately using the **sklearn.datasets make_blobs** function which synthesizes data for analysis by specifying: \n",
    " - The number of samples in the dataset called n_samples, for example n_sample = 200000\n",
    " - The number of columns in the dataset called n_features, for exmaple n_features = 50,\n",
    " - The number of cluster centers called centers, for example centers = 10, \n",
    " - The standard deviation for each cluster called cluster_std, for example cluster_std = 0.2,\n",
    " - The spatial range over which the clusters range, called center_box for example center_box = (-10.0, 10.0), \n",
    " - A seed called random_state, for example random_state = 777"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Kmeans in Batch mode\n",
    "Batch Processing: For small quantities of data, you can input the data all at once using batch processing mode. Batch processing is daal4py's default process mode, so no changes need to be made to your daal4py code in order to run it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Inspect the code cell below and click run ▶ to save the code to a file.\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code.\n",
    "\n",
    "- [Back to Sections](#Back-to-Sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile lab/kmeans_cpu.py\n",
    "#===============================================================================\n",
    "# Copyright 2014-2021 Intel Corporation\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#===============================================================================\n",
    "\n",
    "# daal4py Kmeans example for shared memory systems\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "#import logging\n",
    "#logging.basicConfig(filename='bobOut.log', encoding='utf-8', level=logging.DEBUG)\n",
    "\n",
    "# let's try to use pandas' fast csv reader\n",
    "try:\n",
    "    import pandas\n",
    "\n",
    "    def read_csv(f, c, t=np.float64):\n",
    "        return pandas.read_csv(f, usecols=c, delimiter=',', header=None, dtype=t)\n",
    "except ImportError:\n",
    "    # fall back to numpy loadtxt\n",
    "    def read_csv(f, c, t=np.float64):\n",
    "        return np.loadtxt(f, usecols=c, delimiter=',', ndmin=2)\n",
    "\n",
    "\n",
    "def main(readcsv=read_csv, method='defaultDense'):\n",
    "    infile = \"./data/batch/kmeans_dense.csv\"\n",
    "    nClusters = 20\n",
    "    maxIter = 5\n",
    "            \n",
    "    data = readcsv(infile, range(20))\n",
    "    \n",
    "    kmeans = KMeans(nClusters, init='random', max_iter=300, random_state=0)\n",
    "    y_km = kmeans.fit_predict(data)\n",
    "\n",
    "    print(\"kmeans.labels_\")\n",
    "    print(kmeans.labels_)   \n",
    "    \n",
    "    print(\"kmeans.cluster_centers_\")\n",
    "    print(kmeans.cluster_centers_)\n",
    "\n",
    "result = main()    \n",
    "print('All looks good!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If the Jupyter cells are not responsive or if they error out when you compile the code samples, please restart the Jupyter Kernel: \n",
    "\"Kernel->Restart Kernel and Clear All Outputs\" and compile the code samples again__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Kmeans using Distributed Processing\n",
    "daal4py operates in Single Program Multiple Data (SPMD) style, which means your program is executed on several processes (e.g. similar to MPI). The use of MPI is not required for daal4py’s SPMD-mode to work- all necessary communication and synchronization happens thourgh daal4py. However, it is possible to use daal4py and mpi4py in the same program.\n",
    "\n",
    "Only very minimal changes are needed to your daal4py code to allow daal4py to run on a cluster of workstations. Add this line near the top of the python program to initialize SPMD mode.\n",
    "\n",
    "```\n",
    "daalinit()\n",
    "\n",
    "```\n",
    "Add the distribution parameter to the algorithm construction:\n",
    "\n",
    "```\n",
    "kmi = kmeans_init(10, method=\"plusPlusDense\", distributed=True)\n",
    "\n",
    "```\n",
    "When calling the actual computation each process expects an input file or input array/DataFrame. Your program needs to tell each process which file/array/DataFrame it should operate on.\n",
    "\n",
    "Finally stop the distribution engine:\n",
    "\n",
    "```\n",
    "daalfini()\n",
    "\n",
    "```\n",
    "\n",
    "To actually get it executed on several processes use standard MPI mechanics, like:\n",
    "\n",
    "```\n",
    "mpirun -n 4 python ./kmeans.py\n",
    "\n",
    "```\n",
    "The binaries provided by Intel use the Intel® MPI library, but daal4py can also be compiled for any other MPI implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Inspect the code cell below and click run ▶ to save the code to a file.\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code.\n",
    "\n",
    "- [Back to Sections](#Back-to-Sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/kmeans_spmd.py\n",
    "\n",
    "#===============================================================================\n",
    "# Copyright 2014-2021 Intel Corporation\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#===============================================================================\n",
    "\n",
    "# daal4py Kmeans example for distributed memory systems; SPMD mode\n",
    "# run like this:\n",
    "#    mpirun -n 4 python ./kmeans_spmd.py\n",
    "\n",
    "import daal4py as d4p\n",
    "from numpy import loadtxt\n",
    "\n",
    "\n",
    "def main(method='plusPlusDense'):\n",
    "    infile = \"./data/distributed/kmeans_dense.csv\"\n",
    "    nClusters = 10\n",
    "    maxIter = 25\n",
    "    \n",
    "    print(\"output expected below:\")\n",
    "    \n",
    "    # configure a kmeans-init\n",
    "    init_algo = d4p.kmeans_init(nClusters, method=method, distributed=True)\n",
    "    # Load the data\n",
    "    data = loadtxt(infile, delimiter=',')\n",
    "    # now slice the data,\n",
    "    # it would have been better to read only what we need, of course...\n",
    "    rpp = int(data.shape[0] / d4p.num_procs())\n",
    "    data = data[rpp * d4p.my_procid(): rpp * d4p.my_procid() + rpp, :]\n",
    "\n",
    "    # compute initial centroids\n",
    "    init_result = init_algo.compute(data)\n",
    "    # The results provides the initial centroids\n",
    "    assert init_result.centroids.shape[0] == nClusters\n",
    "\n",
    "    # configure kmeans main object\n",
    "    algo = d4p.kmeans(nClusters, maxIter, distributed=True)\n",
    "    # compute the clusters/centroids\n",
    "    result = algo.compute(data, init_result.centroids)\n",
    "\n",
    "    # Kmeans result objects provide centroids, goalFunction,\n",
    "    # nIterations and objectiveFunction\n",
    "    assert result.centroids.shape[0] == nClusters\n",
    "    assert result.nIterations <= maxIter\n",
    "    # we need an extra call to kmeans to get the assignments\n",
    "    # (not directly supported through parameter assignFlag yet in SPMD mode)\n",
    "    algo = d4p.kmeans(nClusters, 0, assignFlag=True)\n",
    "    # maxIt=0; not distributed, we compute on local data only!\n",
    "    assignments = algo.compute(data, result.centroids).assignments\n",
    "\n",
    "    return (assignments, result)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize SPMD mode\n",
    "    d4p.daalinit()\n",
    "    (assignments, result) = main()\n",
    "    # result is available on all processes - but we print only on root\n",
    "    if d4p.my_procid() == 0:\n",
    "        print(\"\\nFirst 10 cluster assignments:\\n\", assignments[0:10])\n",
    "        print(\"\\nFirst 10 dimensions of centroids:\\n\", result.centroids[:, 0:10])\n",
    "        print(\"\\nObjective function value:\\n\", result.objectiveFunction)\n",
    "        print('All looks good!')\n",
    "    d4p.daalfini()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_kmeans_spmd.sh; if [ -x \"$(command -v qsub)\" ]; then ./q  run_kmeans_spmd.sh; else ./run_kmeans_spmd.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Kmeans targeting GPU\n",
    "\n",
    "1. Inspect the code cell below and click run ▶ to save the code to a file.\n",
    "2. Next run ▶ the cell in the __Build and Run__ section below the code to compile and execute the code.\n",
    "\n",
    "- [Back to Sections](#Back-to-Sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lab/kmeans_gpu.py \n",
    "# daal4py Kmeans example for shared memory systems\n",
    "import pickle\n",
    "import dpctl\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "from daal4py.oneapi import sycl_context\n",
    "from daal4py.oneapi import sycl_buffer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def write_results(resultsDict):\n",
    "    print(\"write_results...\")\n",
    "    file_to_write = open(\"resultsDict.pkl\", \"wb\")\n",
    "    pickle.dump(resultsDict, file_to_write)\n",
    "    file_to_write.close()\n",
    "    print(\"write complete...\")\n",
    "    \n",
    "# let's try to use pandas' fast csv reader\n",
    "try:\n",
    "    import pandas\n",
    "    def read_csv(f, c, t=np.float64):\n",
    "        return pandas.read_csv(f, usecols=c, delimiter=',', header=None, dtype=t)\n",
    "except ImportError:\n",
    "    # fall back to numpy loadtxt\n",
    "    def read_csv(f, c, t=np.float64):\n",
    "        return np.loadtxt(f, usecols=c, delimiter=',', ndmin=2)\n",
    "\n",
    "# Commone code for both CPU and GPU computations\n",
    "def compute(data, nClusters, maxIter, method):    \n",
    "    kmeans = KMeans(nClusters, init='random', max_iter=maxIter, random_state=0)\n",
    "    #kmeans = KMeans(nClusters, random_state=0, init='random', maxIter=5)\n",
    "    y_km = kmeans.fit(data)\n",
    "    pred_y = kmeans.fit_predict(data)\n",
    "    \n",
    "    print(\"kmeans.labels_\")\n",
    "    print(kmeans.labels_)   \n",
    "    \n",
    "    print(\"kmeans.cluster_centers_\")\n",
    "    #print(kmeans.cluster_centers_)    \n",
    "    print(\"\\nFirst 3 cluster centers:\\n\", kmeans.cluster_centers_[0:3])\n",
    "    resultsDict = {}\n",
    "    resultsDict['y_km'] = y_km\n",
    "    resultsDict['pred_y'] = pred_y\n",
    "    resultsDict['kmeans.labels_'] = kmeans.labels_\n",
    "    resultsDict['kmeans.cluster_centers_'] = kmeans.cluster_centers_\n",
    "    return resultsDict\n",
    "\n",
    "\n",
    "# At this moment with sycl we are working only with numpy arrays\n",
    "def to_numpy(data):\n",
    "    try:\n",
    "        from pandas import DataFrame\n",
    "        if isinstance(data, DataFrame):\n",
    "            return np.ascontiguousarray(data.values)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    try:\n",
    "        from scipy.sparse import csr_matrix\n",
    "        if isinstance(data, csr_matrix):\n",
    "            return data.toarray()\n",
    "    except ImportError:\n",
    "        pass\n",
    "    return data\n",
    "\n",
    "\n",
    "def main(readcsv=read_csv, method='randomDense'):\n",
    "    infile = os.path.join('data', 'batch', 'kmeans_dense.csv')\n",
    "    nClusters = 20\n",
    "    maxIter = 5\n",
    "    \n",
    "    print(\"output expected below:\")\n",
    "    \n",
    "    # Load the data\n",
    "    data = readcsv(infile, range(20), t=np.float32)   \n",
    "\n",
    "    # convert to numpy\n",
    "    data = to_numpy(data) \n",
    "\n",
    "    for d in dpctl.get_devices():\n",
    "        if d.is_gpu:\n",
    "            device = dpctl.select_gpu_device()\n",
    "        else:\n",
    "            device = dpctl.select_cpu_device() \n",
    "            \n",
    "    print(device.device_type)\n",
    "    with dpctl.device_context(device):        \n",
    "        resultsDict = compute(data, nClusters, maxIter, method) \n",
    "\n",
    "    write_results(resultsDict)\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()    \n",
    "    print('All looks good!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run\n",
    "Select the cell below and click run ▶ to compile and execute the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 run_kmeans_gpu.sh; if [ -x \"$(command -v qsub)\" ]; then ./q run_kmeans_gpu.sh; else ./run_kmeans_gpu.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If the Jupyter cells are not responsive or if they error out when you compile the code samples, please restart the Jupyter Kernel: \n",
    "\"Kernel->Restart Kernel and Clear All Outputs\" and compile the code samples again__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot kmeans results as computed on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def read_results():\n",
    "    f = open('resultsDict.pkl', 'rb')   # 'rb' for reading binary file\n",
    "    resultsDict = pickle.load(f)     \n",
    "    f.close()  \n",
    "    return(resultsDict)\n",
    "\n",
    "resultsDict = read_results()\n",
    "resultsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "infile = os.path.join('data', 'batch', 'kmeans_dense.csv')\n",
    "# Load the data\n",
    "df = pd.read_csv(infile,  delimiter=',', usecols = range(20) , header=None, dtype=np.float32)\n",
    "X = df.to_numpy()\n",
    "\n",
    "pred_y = resultsDict['pred_y']\n",
    "cluster_centers_ =  resultsDict['kmeans.cluster_centers_']\n",
    "labels_ = resultsDict['kmeans.labels_']\n",
    "c1 = 9\n",
    "c2 = 19\n",
    "plt.title('kmeans cluster centers')\n",
    "plt.scatter(cluster_centers_[:, c1], cluster_centers_[:, c2], s=300, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this module you will have learned the following:\n",
    "* Able to Describe Daal4py and Intel Extension for Scikit-learn methodology in extending scikit-learn optimzation capabilites\n",
    "* Able to Name key imports and function calls to use Intel Extension for Scikit-learn to target Kmeans for use on CPU, GPU and distributed CPU environments\n",
    "* Able to Apply a single Daal4py function to enable Kmeans targeting CPU and GPU using SYCL context\n",
    "* Able to Build a Sklearn implementation of Kmeans targeting CPU and GPU using Intel optimized Sklearn Extensions for Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notices & Disclaimers \n",
    "\n",
    "Intel technologies may require enabled hardware, software or service activation.\n",
    "No product or component can be absolutely secure.\n",
    "\n",
    "Your costs and results may vary.\n",
    "\n",
    "© Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. \n",
    "*Other names and brands may be claimed as the property of others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (Intel® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "525.6px",
    "left": "28px",
    "top": "137.8px",
    "width": "301.09px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
